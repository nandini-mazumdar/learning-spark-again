{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNf2iSrQbm5Q1RwCl+5176c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandini-mazumdar/learning-spark-again/blob/main/Spark_ML_AI_Dan_Sullivan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJtwvbtniF32"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "L-kem_OZiQJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "8V0zX0T0iSRV",
        "outputId": "b58d43e6-f3e3-4a65-e361-b045ec787d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7df1b26c50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6d3be9a621a6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization"
      ],
      "metadata": {
        "id": "4UqGcXm5qBcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MinMaxScaler - attribute values 0 to 1"
      ],
      "metadata": {
        "id": "3tuxLDvYmux9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "metadata": {
        "id": "BVeru5koihB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = spark.createDataFrame([(1, Vectors.dense([10.0, 10000.0, 1.0])),(2, Vectors.dense([20.0, 30000.0, 2.0])),(3, Vectors.dense([30.0, 40000.0, 3.0]))], [\"id\", \"features\"])"
      ],
      "metadata": {
        "id": "CQEE4R08i19e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAA9otd2ki3Q",
        "outputId": "5c416f3e-9e0f-4e45-9383-1747aef263d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0])),\n",
              " Row(id=2, features=DenseVector([20.0, 30000.0, 2.0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create scaler object to transform\n",
        "feature_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"sFeatures\")\n",
        "\n",
        "# transform + fit the vectors into scaled version\n",
        "smodel = feature_scaler.fit(feature_df)\n",
        "\n",
        "# apply transformation to created scaled dataset\n",
        "sFeatures_df = smodel.transform(feature_df)"
      ],
      "metadata": {
        "id": "6tZfUrXpklIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sFeatures_df.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0g3L5FaoDVV",
        "outputId": "f316ced3-bfe2-4903-cf72-487903789202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sFeatures=SparseVector(3, {})),\n",
              " Row(id=2, features=DenseVector([20.0, 30000.0, 2.0]), sFeatures=DenseVector([0.5, 0.6667, 0.5]))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sFeatures_df.select(\"features\", \"sFeatures\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_U9GQu4oH-o",
        "outputId": "cf64d021-8eed-4cfe-b548-4d2c914ea035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+\n",
            "|          features|           sFeatures|\n",
            "+------------------+--------------------+\n",
            "|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "EYVpOXkrqFqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StandardScaler - attribute values range -1 to 1, mean = 0, normal dist"
      ],
      "metadata": {
        "id": "PsXMxJvxm5d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "metadata": {
        "id": "kI88QlkHou2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = spark.createDataFrame([(1, Vectors.dense([10.0, 10000.0, 1.0]),),(2, Vectors.dense([20.0, 30000.0, 3.0]),),(3, Vectors.dense([30.0, 40000.0, 3.0]))], [\"id\", \"features\"])\n",
        "features_df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQvsO3ZkqdZF",
        "outputId": "0bd2581a-c769-4947-98ce-388d53b36c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0])),\n",
              " Row(id=2, features=DenseVector([20.0, 30000.0, 3.0])),\n",
              " Row(id=3, features=DenseVector([30.0, 40000.0, 3.0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create standardized object\n",
        "feature_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"stand_features\", withStd=True, withMean=True)"
      ],
      "metadata": {
        "id": "1DXOhSLHrGg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build standardize model\n",
        "stand_model = feature_stand_scaler.fit(features_df)"
      ],
      "metadata": {
        "id": "xTIOfMDcrkne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply transformation\n",
        "st_features_df = stand_model.transform(features_df)"
      ],
      "metadata": {
        "id": "i2PLkYD7r3Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_features_df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p78PifDOsCMG",
        "outputId": "fe42961c-59dd-458b-ed0a-5686db37cbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), stand_features=DenseVector([-1.0, -1.0911, -1.1547])),\n",
              " Row(id=2, features=DenseVector([20.0, 30000.0, 3.0]), stand_features=DenseVector([0.0, 0.2182, 0.5774])),\n",
              " Row(id=3, features=DenseVector([30.0, 40000.0, 3.0]), stand_features=DenseVector([1.0, 0.8729, 0.5774]))]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st_features_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KySa6MqsE77",
        "outputId": "73176ece-874e-4516-8aa1-31072beca3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|      stand_features|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,3.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bucketize or Partition"
      ],
      "metadata": {
        "id": "J1qE4pQOsuGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Absolute Binning of attributes"
      ],
      "metadata": {
        "id": "idFSTmucnKQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Bucketizer"
      ],
      "metadata": {
        "id": "5o3Y9PEasw0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boundaries for bins\n",
        "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]"
      ],
      "metadata": {
        "id": "dZ6QggPts8lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unbinned_data = [(-800.0,),(-10.5,),(-1.7,),(0.0,),(8.2,),(90.1,)]"
      ],
      "metadata": {
        "id": "_WIPrQwotKbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unbinned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZSCZkTMuiaT",
        "outputId": "1cc72886-8a9a-4eb2-90a2-5751386eaabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binned_df = spark.createDataFrame(unbinned_data, [\"features\"])\n",
        "binned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW3vZHSetpY_",
        "outputId": "335a02b8-0243-4362-a68b-b7be3d4894b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "|   103.4|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-DHdFiQvMju",
        "outputId": "55e5cdc8-8adc-4a29-8667-999c2fd5c770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-inf, -10.0, 0.0, 10.0, inf]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create bucketizer/ slicer object\n",
        "bucketizer = Bucketizer(splits= splits, inputCol=\"features\", outputCol=\"bFeatures\")\n",
        "\n",
        "# apply tranformation\n",
        "bucketized_df = bucketizer.transform(binned_df)\n",
        "bucketized_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaiCkB5it1MY",
        "outputId": "bc0848d2-5e31-4fbc-cc16-2cfcac91c504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|features|bFeatures|\n",
            "+--------+---------+\n",
            "|  -800.0|      0.0|\n",
            "|   -10.5|      0.0|\n",
            "|    -1.7|      1.0|\n",
            "|     0.0|      2.0|\n",
            "|     8.2|      2.0|\n",
            "|    90.1|      3.0|\n",
            "|   103.4|      3.0|\n",
            "+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Text Data"
      ],
      "metadata": {
        "id": "CDug7xwWdKiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer"
      ],
      "metadata": {
        "id": "5UVhsP2XuR5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentances_df = spark.createDataFrame([(1, \"This is an introduction to Spark MLlib\"), (2, \"MLlib includes libraries for classification and regression\"), (3,\"It also contains supporting tools for pipelines\")], [\"id\", \"sentance\"])\n",
        "\n",
        "sentances_df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbUz9mM1dZzf",
        "outputId": "93622c55-4708-4d8f-bc1b-e512e49376d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentance='This is an introduction to Spark MLlib'),\n",
              " Row(id=2, sentance='MLlib includes libraries for classification and regression'),\n",
              " Row(id=3, sentance='It also contains supporting tools for pipelines')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create Tokenizer object\n",
        "sent_token = Tokenizer(inputCol=\"sentance\", outputCol=\"words\")"
      ],
      "metadata": {
        "id": "9GsQonK5eVjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply transform to dataset\n",
        "sent_tokenized_df = sent_token.transform(sentances_df)\n",
        "sent_tokenized_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "uJK9UTRlehxo",
        "outputId": "a1c50742-ea5f-437b-ecb0-6d7ce410e9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---+--------------------+--------------------+\n",
              "| id|            sentance|               words|\n",
              "+---+--------------------+--------------------+\n",
              "|  1|This is an introd...|[this, is, an, in...|\n",
              "|  2|MLlib includes li...|[mllib, includes,...|\n",
              "|  3|It also contains ...|[it, also, contai...|\n",
              "+---+--------------------+--------------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>id</th><th>sentance</th><th>words</th></tr>\n",
              "<tr><td>1</td><td>This is an introd...</td><td>[this, is, an, in...</td></tr>\n",
              "<tr><td>2</td><td>MLlib includes li...</td><td>[mllib, includes,...</td></tr>\n",
              "<tr><td>3</td><td>It also contains ...</td><td>[it, also, contai...</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenized_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDFIiQVPe1wb",
        "outputId": "6d2a67c6-34c8-40c7-ddef-16309f9d6dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| id|            sentance|               words|\n",
            "+---+--------------------+--------------------+\n",
            "|  1|This is an introd...|[this, is, an, in...|\n",
            "|  2|MLlib includes li...|[mllib, includes,...|\n",
            "|  3|It also contains ...|[it, also, contai...|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency - Inverse Document Frequency\n",
        "### TF-IDF"
      ],
      "metadata": {
        "id": "iLC3Hr0lfMYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF"
      ],
      "metadata": {
        "id": "5j9qaKZxfEEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentances_df.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0j4oY6UfrqR",
        "outputId": "9a386516-55f9-469a-857e-8be855425b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentance='This is an introduction to Spark MLlib')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenized_df.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMEGgT_Ffv2P",
        "outputId": "ef04b9a0-2161-4ef4-ebc3-59d251a5109a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentance='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hashing TF"
      ],
      "metadata": {
        "id": "jFyuI1QxlEOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hashing_df = HashingTF(inputCol=\"words\",outputCol=\"raw_features\",numFeatures=20)"
      ],
      "metadata": {
        "id": "NCFNhCYvk9oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_hashingTF = hashing_df.transform(sent_tokenized_df)"
      ],
      "metadata": {
        "id": "jL2W5GbSlSMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_hashingTF.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB0crvF5llVq",
        "outputId": "8c70bb38-57a0-485b-e33f-83ad559be35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentance='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], raw_features=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the df acc to freq of words\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"IDF_features\")"
      ],
      "metadata": {
        "id": "Qy5443HtloKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit\n",
        "idf_model = idf.fit(sent_hashingTF)"
      ],
      "metadata": {
        "id": "_6xQGO56mDP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply transformation and get new idf_features column\n",
        "tf_idf_df = idf_model.transform(sent_hashingTF)"
      ],
      "metadata": {
        "id": "0G6y7IVEmNdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_df.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhpOTRfWmZbl",
        "outputId": "4f8c67fc-19e3-4d70-e0d5-9fa39dfb5bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentance='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], raw_features=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}), IDF_features=SparseVector(20, {6: 0.5754, 8: 0.2877, 9: 0.6931, 10: 0.6931, 13: 0.6931, 15: 0.2877}))]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refer Intro to Spark-Chap 5 to continue to ML tasks\n",
        "https://colab.research.google.com/drive/1zI9fV-i2besK1O2qfJ0Z24W6OSN5URW-?usp=sharing"
      ],
      "metadata": {
        "id": "8raGPpSSnxOd"
      }
    }
  ]
}